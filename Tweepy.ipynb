{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install tweepy\n",
    "# !pip install git+https://github.com/tweepy/tweepy.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f140842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2587a6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if there's .env with BEARER_TOKEN key\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "ae31e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"Tweepy\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = logging.FileHandler(filename=\"tweepy.log\")\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c7042c",
   "metadata": {},
   "source": [
    "Data extraction closely following this [example from tweepy](https://github.com/tweepy/tweepy/blob/master/examples/API_v2/search_recent_tweets.py)\n",
    "\n",
    "Pagination closely following this [pagination page from tweepy](https://docs.tweepy.org/en/stable/v2_pagination.html) (my code didn't use `flatten` because it doesn't have the `includes` attr necessary to extract retweets; though, because of that the data isn't as much richer)\n",
    "\n",
    "Check out the [recent search API](https://docs.tweepy.org/en/stable/client.html#tweepy.Client.search_recent_tweets) for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "53b7b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=BEARER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7549e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_full_text(tweet, includes):\n",
    "    \"\"\"\n",
    "    Full text can only be found on the original tweet.\n",
    "    but, the data also contains RTs in which text are truncated.\n",
    "    so, try to detect whether a tweet is an RT and extract text from its original\n",
    "    \"\"\"\n",
    "    # if tweet is an RT, search for its original in `includes`\n",
    "    if tweet.referenced_tweets and \\\n",
    "        tweet.referenced_tweets[0].type == \"retweeted\":\n",
    "        return list(\n",
    "                filter(lambda x: x.id == tweet.referenced_tweets[0].id,\n",
    "                       includes['tweets'])\n",
    "                ).pop().text\n",
    "    \n",
    "    # tweet is original, just return the text\n",
    "    return tweet.text\n",
    "\n",
    "\n",
    "def extract_tweet_data(tweet):\n",
    "    data = []\n",
    "    for t in tweet.data:\n",
    "        d = {\n",
    "            \"author_id\": t.author_id,\n",
    "            \"id\": t.id,\n",
    "            \"in_reply_to_user_id\": t.in_reply_to_user_id,\n",
    "            # extract date wtih timezone\n",
    "            # https://stackoverflow.com/a/48725037/8996974\n",
    "            \"created_at\": t.created_at.strftime('%Y-%m-%d %H:%M:%S.%f'),\n",
    "            \"lang\": t.lang,\n",
    "            \"text\": _extract_full_text(t, tweet.includes),\n",
    "            \"possibly_sensitive\": t.possibly_sensitive,\n",
    "            \"source\": t.source,\n",
    "        }\n",
    "        data.append(d)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "5520c71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump(name, tweets):\n",
    "    # include the .json format in name\n",
    "    assert name[-5:] == '.json'\n",
    "    \n",
    "    # overwrite current file if exist\n",
    "    with open(name, \"w\") as f:\n",
    "        f.write(\"\")\n",
    "    \n",
    "    with open(name, \"a\") as f:\n",
    "        for tweet in tweets:\n",
    "            json.dump(tweet, f, indent=4)\n",
    "    \n",
    "    print(f\"{name} dumped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd8581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREELY EDIT THIS\n",
    "def _runner(dump_filename):\n",
    "    data = []\n",
    "    for tweet in tweepy.Paginator(client.search_recent_tweets,\n",
    "                                  query,\n",
    "                                  expansions=expansions,\n",
    "                                  tweet_fields=tweet_fields,\n",
    "                                  max_results=100,\n",
    "                                  limit=180):\n",
    "        ex = extract_tweet_data(tweet)\n",
    "        data.extend(ex)\n",
    "    dump(dump_filename, data)\n",
    "\n",
    "        \n",
    "# https://developer.twitter.com/en/docs/twitter-api/rate-limits\n",
    "# Requests limit per 15-minute for recent search is 180 per user\n",
    "FIFTEEN_MINUTES = 900  # in seconds\n",
    "\n",
    "def run(query, tweet_fields, expansions):\n",
    "    target_tweets_count = 100_000\n",
    "    target_tweets_progress = 0\n",
    "    \n",
    "    while target_tweets_progress < target_tweets_count:\n",
    "        start = time.time()\n",
    "        print(f\"Processing part-{target_tweets_progress}\")\n",
    "\n",
    "        dump_filename = f\"{target_tweets_progress}.json\"\n",
    "        _runner(dump_filename)\n",
    "        \n",
    "        if (time.time() - start) < FIFTEEN_MINUTES:\n",
    "            # if finished earlier before 15 minutes; sleep (add 10 as buffer)\n",
    "            sleep_for = (time.time() - start) + 10\n",
    "            time.sleep(sleep_for)\n",
    "            \n",
    "        target_tweets_progress += (180 * 100)\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6664f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"bitcoin OR btc OR ethereum OR eth OR crypto\"\n",
    "tweet_fields = \"author_id,id,in_reply_to_user_id,created_at,lang,text,possibly_sensitive,source,referenced_tweets\"\n",
    "expansions = \"referenced_tweets.id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1e6c1432",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(query, tweet_fields, expansions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
